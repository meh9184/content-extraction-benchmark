# The Evaluation

The Evaluation calculates performance based on the output of content extraction algorithm and the answers generated by the user.
There are three basic Performance metric:

> (1) DOM tree edit distance   
> (2) area intersection   
> (3) word/tag token similarity. 

Another metric can be added arbitrarily.

## Usage
```
$ cd evaluation
$ python manage.py runserver
```

## End Point

* Resoueces
  * /sites
  * /pages
  * /answers
  * /predicts
  * /performances
  * /users
  * /groups
  * /calculations
* Pages
  * /main
  * /performance
  * /performance_detail